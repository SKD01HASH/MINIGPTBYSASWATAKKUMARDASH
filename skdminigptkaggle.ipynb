{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762c40f1",
   "metadata": {
    "id": "bFz5lAIQ8CY8",
    "papermill": {
     "duration": 0.011423,
     "end_time": "2023-07-06T08:30:55.014363",
     "exception": false,
     "start_time": "2023-07-06T08:30:55.002940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## My Gpt:\n",
    "- Reference:-  Zero To Hero video on GPT.\n",
    "- Model by:- Saswata Kumar Dash\n",
    "- Video link which I followed:-\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy\n",
    "- I have also added the GPT model which was not there in the video and made some changes I have added two bigram models here as you can see and in end I have added the gpt model also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8b14ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:30:55.038253Z",
     "iopub.status.busy": "2023-07-06T08:30:55.037451Z",
     "iopub.status.idle": "2023-07-06T08:30:57.637784Z",
     "shell.execute_reply": "2023-07-06T08:30:57.636666Z"
    },
    "id": "iwlDKr-38kFr",
    "outputId": "a1362bca-4771-453c-c74a-ee98591ff78a",
    "papermill": {
     "duration": 2.614965,
     "end_time": "2023-07-06T08:30:57.640232",
     "exception": false,
     "start_time": "2023-07-06T08:30:55.025267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-06 08:30:56--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: ‘input.txt’\r\n",
      "\r\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \r\n",
      "\r\n",
      "2023-07-06 08:30:56 (28.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\r\n",
      "\r\n",
      "--2023-07-06 08:30:57--  https://raw.githubusercontent.com/karpathy/ng-video-lecture/master/more.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 10001 (9.8K) [text/plain]\r\n",
      "Saving to: ‘more.txt’\r\n",
      "\r\n",
      "more.txt            100%[===================>]   9.77K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2023-07-06 08:30:57 (63.9 MB/s) - ‘more.txt’ saved [10001/10001]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Getting the dataset to train on.\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "!wget https://raw.githubusercontent.com/karpathy/ng-video-lecture/master/more.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb20a72",
   "metadata": {
    "id": "i-NdkwgR85sN",
    "papermill": {
     "duration": 0.011303,
     "end_time": "2023-07-06T08:30:57.663162",
     "exception": false,
     "start_time": "2023-07-06T08:30:57.651859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Read to inspect part:-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf87e673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:30:57.687801Z",
     "iopub.status.busy": "2023-07-06T08:30:57.686936Z",
     "iopub.status.idle": "2023-07-06T08:30:57.695844Z",
     "shell.execute_reply": "2023-07-06T08:30:57.694115Z"
    },
    "id": "tde5bZqg9CIl",
    "outputId": "53a83b32-1e85-4aa5-ebe5-fadc7d4a4b26",
    "papermill": {
     "duration": 0.02359,
     "end_time": "2023-07-06T08:30:57.698037",
     "exception": false,
     "start_time": "2023-07-06T08:30:57.674447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "# now lets's look at the first 1000 characters in the dataset\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7ad4f",
   "metadata": {
    "id": "_HqzRMT495HC",
    "papermill": {
     "duration": 0.011111,
     "end_time": "2023-07-06T08:30:57.720898",
     "exception": false,
     "start_time": "2023-07-06T08:30:57.709787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's take all the unique characters that occur in this text. ~skd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e7e6a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:30:57.746217Z",
     "iopub.status.busy": "2023-07-06T08:30:57.745343Z",
     "iopub.status.idle": "2023-07-06T08:30:57.767634Z",
     "shell.execute_reply": "2023-07-06T08:30:57.766354Z"
    },
    "id": "nix0scoW-Jdp",
    "outputId": "eb317e81-fe0c-4946-a0b3-c154ab9449d6",
    "papermill": {
     "duration": 0.037185,
     "end_time": "2023-07-06T08:30:57.769886",
     "exception": false,
     "start_time": "2023-07-06T08:30:57.732701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5370837",
   "metadata": {
    "id": "lAIso2kL-YsZ",
    "papermill": {
     "duration": 0.011506,
     "end_time": "2023-07-06T08:30:57.792848",
     "exception": false,
     "start_time": "2023-07-06T08:30:57.781342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's create a mapping from characters to integers.~ SKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288c9b58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:30:57.817794Z",
     "iopub.status.busy": "2023-07-06T08:30:57.817504Z",
     "iopub.status.idle": "2023-07-06T08:30:57.823980Z",
     "shell.execute_reply": "2023-07-06T08:30:57.823111Z"
    },
    "id": "BeEjWadh-iGL",
    "outputId": "5e935b6b-dc7d-4d0f-9216-6212d62cf175",
    "papermill": {
     "duration": 0.021935,
     "end_time": "2023-07-06T08:30:57.826319",
     "exception": false,
     "start_time": "2023-07-06T08:30:57.804384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# the above part is an encoder that will take a string and the output  a list of integers ~skd\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "# the above part is a decoder which the vice versa of the encoder. ~skd\n",
    "#let's take an example of it ~skd\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a2caf",
   "metadata": {
    "id": "7JNrWGq3A3vl",
    "papermill": {
     "duration": 0.011501,
     "end_time": "2023-07-06T08:30:57.849302",
     "exception": false,
     "start_time": "2023-07-06T08:30:57.837801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's encode the entire text dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0617f94f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:30:57.874001Z",
     "iopub.status.busy": "2023-07-06T08:30:57.873078Z",
     "iopub.status.idle": "2023-07-06T08:31:01.272260Z",
     "shell.execute_reply": "2023-07-06T08:31:01.271266Z"
    },
    "id": "z5IXg8a5BfSx",
    "outputId": "655a9af5-c1fe-4c8d-db5d-07c2a543887b",
    "papermill": {
     "duration": 3.414301,
     "end_time": "2023-07-06T08:31:01.274969",
     "exception": false,
     "start_time": "2023-07-06T08:30:57.860668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# here we are importing the pytorch library from pytorch.org.\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596aacff",
   "metadata": {
    "id": "EHQb2_Q4MjT2",
    "papermill": {
     "duration": 0.011668,
     "end_time": "2023-07-06T08:31:01.298513",
     "exception": false,
     "start_time": "2023-07-06T08:31:01.286845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's split up the data into train and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb505d10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:01.324441Z",
     "iopub.status.busy": "2023-07-06T08:31:01.322718Z",
     "iopub.status.idle": "2023-07-06T08:31:01.332314Z",
     "shell.execute_reply": "2023-07-06T08:31:01.331342Z"
    },
    "id": "y8rFUBkxJefL",
    "outputId": "0adfddd4-8159-4ebe-e804-68d22de5be22",
    "papermill": {
     "duration": 0.024044,
     "end_time": "2023-07-06T08:31:01.334219",
     "exception": false,
     "start_time": "2023-07-06T08:31:01.310175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4405452a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:01.359705Z",
     "iopub.status.busy": "2023-07-06T08:31:01.359019Z",
     "iopub.status.idle": "2023-07-06T08:31:01.368725Z",
     "shell.execute_reply": "2023-07-06T08:31:01.367304Z"
    },
    "id": "oCCIMWwWMqyB",
    "outputId": "9125b8c6-7619-401c-ee69-0a6790411e6f",
    "papermill": {
     "duration": 0.02462,
     "end_time": "2023-07-06T08:31:01.371097",
     "exception": false,
     "start_time": "2023-07-06T08:31:01.346477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60922b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:01.406162Z",
     "iopub.status.busy": "2023-07-06T08:31:01.404481Z",
     "iopub.status.idle": "2023-07-06T08:31:01.435976Z",
     "shell.execute_reply": "2023-07-06T08:31:01.435042Z"
    },
    "id": "z4C2drlGKTxn",
    "outputId": "a09fa6a4-5c19-4a3f-dbf2-bdee085a71b3",
    "papermill": {
     "duration": 0.046947,
     "end_time": "2023-07-06T08:31:01.438257",
     "exception": false,
     "start_time": "2023-07-06T08:31:01.391310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "def get_batch(spilt):\n",
    "  data = train_data if spilt == 'train' else val_data\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  return x, y\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b,t]\n",
    "    print(f\"when input is {context.tolist()} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86de9b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:01.463389Z",
     "iopub.status.busy": "2023-07-06T08:31:01.462853Z",
     "iopub.status.idle": "2023-07-06T08:31:01.468843Z",
     "shell.execute_reply": "2023-07-06T08:31:01.467559Z"
    },
    "id": "ipkSrHKLQCG6",
    "outputId": "ce60dac5-f031-4333-f616-c57fbf0eac62",
    "papermill": {
     "duration": 0.02068,
     "end_time": "2023-07-06T08:31:01.470785",
     "exception": false,
     "start_time": "2023-07-06T08:31:01.450105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "795fa25d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:01.499250Z",
     "iopub.status.busy": "2023-07-06T08:31:01.498480Z",
     "iopub.status.idle": "2023-07-06T08:31:01.604468Z",
     "shell.execute_reply": "2023-07-06T08:31:01.603277Z"
    },
    "id": "5ioCAI5gQGN6",
    "outputId": "ba9dccf6-80e0-47cb-b04d-d41e11683c97",
    "papermill": {
     "duration": 0.121247,
     "end_time": "2023-07-06T08:31:01.606549",
     "exception": false,
     "start_time": "2023-07-06T08:31:01.485302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b894dd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:01.657116Z",
     "iopub.status.busy": "2023-07-06T08:31:01.656654Z",
     "iopub.status.idle": "2023-07-06T08:31:01.965038Z",
     "shell.execute_reply": "2023-07-06T08:31:01.962608Z"
    },
    "id": "RXGhQKk_1RHw",
    "outputId": "9d575d03-906c-40c3-bb8d-400655b5ef7d",
    "papermill": {
     "duration": 0.335036,
     "end_time": "2023-07-06T08:31:01.967221",
     "exception": false,
     "start_time": "2023-07-06T08:31:01.632185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.587916374206543\n",
      "\n",
      "xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
      "3Q&sGlvHQ?mqSq-eON\n",
      "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
      "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNL&A'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
      "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC\n",
      "&WDdP!Ko,px\n",
      "x\n",
      "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
      "pTErIBjx;JKIgoCnLGXrJSP!AU-AcbczR?\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "for steps in range(100):\n",
    "  xb,yb = get_batch('train')\n",
    "  logits, loss = m(xb,yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "print(loss.item())\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af3a11c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:01.994300Z",
     "iopub.status.busy": "2023-07-06T08:31:01.993438Z",
     "iopub.status.idle": "2023-07-06T08:31:02.012016Z",
     "shell.execute_reply": "2023-07-06T08:31:02.010919Z"
    },
    "id": "jmRsvxirDtYL",
    "outputId": "73be7537-5642-40a0-ce1e-0f212bb06c92",
    "papermill": {
     "duration": 0.034355,
     "end_time": "2023-07-06T08:31:02.014196",
     "exception": false,
     "start_time": "2023-07-06T08:31:01.979841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18315b82",
   "metadata": {
    "id": "gm9th0-DFXyu",
    "papermill": {
     "duration": 0.012449,
     "end_time": "2023-07-06T08:31:02.038975",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.026526",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c45efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.065471Z",
     "iopub.status.busy": "2023-07-06T08:31:02.064650Z",
     "iopub.status.idle": "2023-07-06T08:31:02.072919Z",
     "shell.execute_reply": "2023-07-06T08:31:02.071754Z"
    },
    "id": "POetn0zuEhKe",
    "outputId": "5a635238-6198-4126-9d40-e22f6a28f875",
    "papermill": {
     "duration": 0.023503,
     "end_time": "2023-07-06T08:31:02.074908",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.051405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd3425d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.101298Z",
     "iopub.status.busy": "2023-07-06T08:31:02.100599Z",
     "iopub.status.idle": "2023-07-06T08:31:02.107420Z",
     "shell.execute_reply": "2023-07-06T08:31:02.106588Z"
    },
    "id": "qEHpfIuUHCGw",
    "papermill": {
     "duration": 0.022046,
     "end_time": "2023-07-06T08:31:02.109301",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.087255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "004024c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.135570Z",
     "iopub.status.busy": "2023-07-06T08:31:02.134704Z",
     "iopub.status.idle": "2023-07-06T08:31:02.145998Z",
     "shell.execute_reply": "2023-07-06T08:31:02.145070Z"
    },
    "id": "_oOVyAdqH3Lx",
    "outputId": "542bb720-d561-4119-ef3f-729e609a8769",
    "papermill": {
     "duration": 0.026517,
     "end_time": "2023-07-06T08:31:02.148037",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.121520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd5020e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.175042Z",
     "iopub.status.busy": "2023-07-06T08:31:02.174205Z",
     "iopub.status.idle": "2023-07-06T08:31:02.183172Z",
     "shell.execute_reply": "2023-07-06T08:31:02.182256Z"
    },
    "id": "byTbBEh1IC__",
    "outputId": "5d51e9cc-697f-48b3-f482-54a5ed7e21f4",
    "papermill": {
     "duration": 0.024491,
     "end_time": "2023-07-06T08:31:02.185226",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.160735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 3: using softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e295e2ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.212080Z",
     "iopub.status.busy": "2023-07-06T08:31:02.211342Z",
     "iopub.status.idle": "2023-07-06T08:31:02.224297Z",
     "shell.execute_reply": "2023-07-06T08:31:02.223352Z"
    },
    "id": "TR-lkuOJW6M1",
    "outputId": "47d3dd65-7006-4643-8042-5186cc1e9265",
    "papermill": {
     "duration": 0.028414,
     "end_time": "2023-07-06T08:31:02.226256",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.197842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "wei =  q @ k.transpose(-2, -1)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "870459ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.253435Z",
     "iopub.status.busy": "2023-07-06T08:31:02.252684Z",
     "iopub.status.idle": "2023-07-06T08:31:02.259254Z",
     "shell.execute_reply": "2023-07-06T08:31:02.258321Z"
    },
    "id": "SZnY2ZxajUyf",
    "outputId": "8ffc9844-94c2-4ee9-821c-58accab1930a",
    "papermill": {
     "duration": 0.022119,
     "end_time": "2023-07-06T08:31:02.261226",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.239107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005df24",
   "metadata": {
    "id": "cqWIPUCMjfxp",
    "papermill": {
     "duration": 0.012568,
     "end_time": "2023-07-06T08:31:02.286659",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.274091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note it :\n",
    "Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "self-attention just means that the keys and values are produced from the same source as queries.In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module) \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2379b193",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.314666Z",
     "iopub.status.busy": "2023-07-06T08:31:02.313266Z",
     "iopub.status.idle": "2023-07-06T08:31:02.324043Z",
     "shell.execute_reply": "2023-07-06T08:31:02.323113Z"
    },
    "id": "pmlV2n7jkIu6",
    "outputId": "62c0aeb1-29f1-486f-9a85-366bd025e995",
    "papermill": {
     "duration": 0.026402,
     "end_time": "2023-07-06T08:31:02.325964",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.299562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "k.var()\n",
    "q.var()\n",
    "wei.var()\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3debcf07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.353613Z",
     "iopub.status.busy": "2023-07-06T08:31:02.352837Z",
     "iopub.status.idle": "2023-07-06T08:31:02.365923Z",
     "shell.execute_reply": "2023-07-06T08:31:02.364931Z"
    },
    "id": "ongI1oWQma_u",
    "outputId": "dbf4c1db-5aba-434b-aa4b-9b58a10396e1",
    "papermill": {
     "duration": 0.028872,
     "end_time": "2023-07-06T08:31:02.367884",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.339012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70dc0021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.396538Z",
     "iopub.status.busy": "2023-07-06T08:31:02.395712Z",
     "iopub.status.idle": "2023-07-06T08:31:02.403125Z",
     "shell.execute_reply": "2023-07-06T08:31:02.402185Z"
    },
    "id": "LiN9lkx3pzEq",
    "outputId": "958505c1-4f21-478a-ba72-ab691fe846ba",
    "papermill": {
     "duration": 0.023784,
     "end_time": "2023-07-06T08:31:02.405136",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.381352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "095155d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.433499Z",
     "iopub.status.busy": "2023-07-06T08:31:02.432748Z",
     "iopub.status.idle": "2023-07-06T08:31:02.440907Z",
     "shell.execute_reply": "2023-07-06T08:31:02.439856Z"
    },
    "id": "e_7MSiTap2Ga",
    "outputId": "b77e1d7d-2a3a-49b4-9e0d-e391289781d7",
    "papermill": {
     "duration": 0.024323,
     "end_time": "2023-07-06T08:31:02.442953",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.418630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7f045",
   "metadata": {
    "id": "9e77qX2ap4ri",
    "papermill": {
     "duration": 0.013068,
     "end_time": "2023-07-06T08:31:02.469456",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.456388",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74cc6750",
   "metadata": {
    "id": "uqwYBauIrWx4",
    "papermill": {
     "duration": 0.015192,
     "end_time": "2023-07-06T08:31:02.498163",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.482971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "THE BIAGRAM MODEL WHOLE CODE WHICH WE DID RIGHT NOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "328309bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:02.527388Z",
     "iopub.status.busy": "2023-07-06T08:31:02.527008Z",
     "iopub.status.idle": "2023-07-06T08:31:15.390837Z",
     "shell.execute_reply": "2023-07-06T08:31:15.389659Z"
    },
    "id": "AvS9WswisG6R",
    "outputId": "53c87beb-ba66-41f0-b8f3-ebd6529c1b03",
    "papermill": {
     "duration": 12.88145,
     "end_time": "2023-07-06T08:31:15.392885",
     "exception": false,
     "start_time": "2023-07-06T08:31:02.511435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7305, val loss 4.7241\n",
      "step 300: train loss 2.8110, val loss 2.8249\n",
      "step 600: train loss 2.5434, val loss 2.5682\n",
      "step 900: train loss 2.4932, val loss 2.5088\n",
      "step 1200: train loss 2.4863, val loss 2.5035\n",
      "step 1500: train loss 2.4665, val loss 2.4921\n",
      "step 1800: train loss 2.4683, val loss 2.4936\n",
      "step 2100: train loss 2.4696, val loss 2.4846\n",
      "step 2400: train loss 2.4638, val loss 2.4879\n",
      "step 2700: train loss 2.4738, val loss 2.4911\n",
      "\n",
      "\n",
      "\n",
      "CEThik brid owindakis b, bth\n",
      "\n",
      "HAPet bobe d e.\n",
      "S:\n",
      "O:3 my d?\n",
      "LUCous:\n",
      "Wanthar u qur, t.\n",
      "War dXENDoate awice my.\n",
      "\n",
      "Hastarom oroup\n",
      "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.\n",
      "Swanousel lind me l.\n",
      "HAshe ce hiry:\n",
      "Supr aisspllw y.\n",
      "Hentofu n Boopetelaves\n",
      "MPOLI s, d mothakleo Windo whth eisbyo the m dourive we higend t so mower; te\n",
      "\n",
      "AN ad nterupt f s ar igr t m:\n",
      "\n",
      "Thin maleronth,\n",
      "Mad\n",
      "RD:\n",
      "\n",
      "WISo myrangoube!\n",
      "KENob&y, wardsal thes ghesthinin couk ay aney IOUSts I&fr y ce.\n",
      "J\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077efb5",
   "metadata": {
    "id": "NXGQzQ5-sm-V",
    "papermill": {
     "duration": 0.013957,
     "end_time": "2023-07-06T08:31:15.421498",
     "exception": false,
     "start_time": "2023-07-06T08:31:15.407541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The translation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b21522b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:31:15.451999Z",
     "iopub.status.busy": "2023-07-06T08:31:15.451311Z",
     "iopub.status.idle": "2023-07-06T08:34:59.991358Z",
     "shell.execute_reply": "2023-07-06T08:34:59.990066Z"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "d7546bc2-c0d1-48d3-c900-9fd4cb111cdf",
    "papermill": {
     "duration": 224.558143,
     "end_time": "2023-07-06T08:34:59.993997",
     "exception": false,
     "start_time": "2023-07-06T08:31:15.435854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5058\n",
      "step 300: train loss 2.4198, val loss 2.4337\n",
      "step 400: train loss 2.3499, val loss 2.3561\n",
      "step 500: train loss 2.2963, val loss 2.3127\n",
      "step 600: train loss 2.2408, val loss 2.2499\n",
      "step 700: train loss 2.2057, val loss 2.2191\n",
      "step 800: train loss 2.1636, val loss 2.1869\n",
      "step 900: train loss 2.1241, val loss 2.1507\n",
      "step 1000: train loss 2.1025, val loss 2.1294\n",
      "step 1100: train loss 2.0696, val loss 2.1187\n",
      "step 1200: train loss 2.0376, val loss 2.0789\n",
      "step 1300: train loss 2.0242, val loss 2.0641\n",
      "step 1400: train loss 1.9917, val loss 2.0361\n",
      "step 1500: train loss 1.9703, val loss 2.0313\n",
      "step 1600: train loss 1.9626, val loss 2.0489\n",
      "step 1700: train loss 1.9414, val loss 2.0140\n",
      "step 1800: train loss 1.9078, val loss 1.9946\n",
      "step 1900: train loss 1.9081, val loss 1.9885\n",
      "step 2000: train loss 1.8840, val loss 1.9970\n",
      "step 2100: train loss 1.8713, val loss 1.9759\n",
      "step 2200: train loss 1.8609, val loss 1.9634\n",
      "step 2300: train loss 1.8559, val loss 1.9525\n",
      "step 2400: train loss 1.8406, val loss 1.9427\n",
      "step 2500: train loss 1.8169, val loss 1.9436\n",
      "step 2600: train loss 1.8236, val loss 1.9388\n",
      "step 2700: train loss 1.8143, val loss 1.9343\n",
      "step 2800: train loss 1.8024, val loss 1.9204\n",
      "step 2900: train loss 1.8055, val loss 1.9330\n",
      "step 3000: train loss 1.7952, val loss 1.9214\n",
      "step 3100: train loss 1.7714, val loss 1.9174\n",
      "step 3200: train loss 1.7530, val loss 1.9111\n",
      "step 3300: train loss 1.7569, val loss 1.9033\n",
      "step 3400: train loss 1.7539, val loss 1.8968\n",
      "step 3500: train loss 1.7354, val loss 1.8931\n",
      "step 3600: train loss 1.7298, val loss 1.8935\n",
      "step 3700: train loss 1.7273, val loss 1.8823\n",
      "step 3800: train loss 1.7195, val loss 1.8927\n",
      "step 3900: train loss 1.7212, val loss 1.8756\n",
      "step 4000: train loss 1.7111, val loss 1.8605\n",
      "step 4100: train loss 1.7118, val loss 1.8744\n",
      "step 4200: train loss 1.7086, val loss 1.8676\n",
      "step 4300: train loss 1.7025, val loss 1.8463\n",
      "step 4400: train loss 1.7051, val loss 1.8635\n",
      "step 4500: train loss 1.6880, val loss 1.8466\n",
      "step 4600: train loss 1.6871, val loss 1.8313\n",
      "step 4700: train loss 1.6829, val loss 1.8418\n",
      "step 4800: train loss 1.6657, val loss 1.8385\n",
      "step 4900: train loss 1.6710, val loss 1.8401\n",
      "step 4999: train loss 1.6666, val loss 1.8198\n",
      "\n",
      "And they bride will that were madie;\n",
      "Thou but tarth that weal tauge art that us hath but redilaccao,\n",
      "Away, my fearst, where me\n",
      "Yout proof it.\n",
      "But this nowle, and if ensent, will is the overtage.\n",
      "\n",
      "WARGENV:\n",
      "All just, lest dise of thus queen,\n",
      "Now up have the tyban's nou nor\n",
      "To this lost my liked me, and on in on him evicks to the\n",
      "Murth high, is he poor of his but\n",
      "pray nobrurt for treagint me.\n",
      "\n",
      "PRIARE:\n",
      "Uncta, affarry, I hom.\n",
      "\n",
      "HENRY BOLINGBROY:\n",
      "\n",
      "Shose Warwick, stavoin courtear tear repts I\n",
      "am the while me: ank, I sum horse of bream of a cempres-spend;\n",
      "But with reason; and being wilt weep on,\n",
      "Thou confessyy stratchs lome\n",
      "This sontly evers.\n",
      "\n",
      "LUCIO:\n",
      "Abere ove Lady Canscatom's countent boy\n",
      "Of the hasth a gRilates:y, contly gravent\n",
      "Whose is deed, is xughion the bear\n",
      "that fraving with some. Do self For these lack.\n",
      "\n",
      "PRAMININGHAS IS:\n",
      "No?\n",
      "Their his divinger, do this mister,\n",
      "Houstimed the now the summon approad'd whose no crow out.\n",
      "\n",
      "ESBRALANA:\n",
      "Now, thy getry is.\n",
      "\n",
      "BENSINCE:\n",
      "You the glear Gly thee, our her and brune.\n",
      "\n",
      "POMPEY:\n",
      "Own no; go whill conve, do windlisand, and to you bring!\n",
      "\n",
      "AMINIUS:\n",
      "Gitters to tears his in infect high him a wordn,\n",
      "And left braned his foIndsmy a canies to may incerance\n",
      "In to dot me attele true exre brotengity.\n",
      "\n",
      "DUKE VINGING\n",
      "BRADTUS:\n",
      "Most'st He whertuse me;\n",
      "For there'er the endseman the vown;\n",
      "Mad there to but but his forhward\n",
      "But lothy doth'dl they sich the beguts\n",
      "Good vonburt Rithmons, the baelings whom the deenums ome,\n",
      "Sirral you there plains. Me our son one old,\n",
      "Then them signnest what them wars, evis;\n",
      "River thou a--a scace:\n",
      "Out.\n",
      "\n",
      "PORALNIA:\n",
      "By nurge, ristrake, you all prevons year try.\n",
      "\n",
      "BENVOLIO:\n",
      "Now.\n",
      "From I may wound arm that sure give,\n",
      "Befock, mah, sinstats o' be, God the have blay not,\n",
      "That bring begire man I heav toge of the hamt,\n",
      "Threobly tall weak the sweett aftears' of trust:\n",
      "Ner honerous 'lift he putilt\n",
      "With is a scause; whom now.\n",
      "\n",
      "CORIOLPHEY:\n",
      "May, many and, and, Jom. It\n",
      "Take of to, Jury Roman!\n",
      "Nor brien shown's fortunded such to firther,\n",
      "For an wh\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2ce74",
   "metadata": {
    "id": "o3WQ7BN7p6aN",
    "papermill": {
     "duration": 0.018246,
     "end_time": "2023-07-06T08:35:00.030552",
     "exception": false,
     "start_time": "2023-07-06T08:35:00.012306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "THE GPT MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdfaec75",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-07-06T08:35:00.068356Z",
     "iopub.status.busy": "2023-07-06T08:35:00.067996Z",
     "iopub.status.idle": "2023-07-06T09:05:01.208603Z",
     "shell.execute_reply": "2023-07-06T09:05:01.206250Z"
    },
    "id": "NG3y1n2Ip5_Y",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "fb184d78-71e8-4719-dfc9-f308f6c33a33",
    "papermill": {
     "duration": 1801.162217,
     "end_time": "2023-07-06T09:05:01.210732",
     "exception": false,
     "start_time": "2023-07-06T08:35:00.048515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n",
      "step 0: train loss 4.2221, val loss 4.2306\n",
      "step 500: train loss 1.7587, val loss 1.9086\n",
      "step 1000: train loss 1.3919, val loss 1.5998\n",
      "step 1500: train loss 1.2673, val loss 1.5301\n",
      "step 2000: train loss 1.1904, val loss 1.5142\n",
      "step 2500: train loss 1.1194, val loss 1.5024\n",
      "step 3000: train loss 1.0738, val loss 1.4949\n",
      "step 3500: train loss 1.0213, val loss 1.5150\n",
      "step 4000: train loss 0.9618, val loss 1.5183\n",
      "step 4500: train loss 0.9087, val loss 1.5446\n",
      "step 4999: train loss 0.8543, val loss 1.5633\n",
      "\n",
      "Which we thank they shall follow men;\n",
      "That bear this news ere his tooth doth chase,\n",
      "More than a doth to eunmoon on.\n",
      "\n",
      "CAMILLO:\n",
      "Tell him,--\n",
      "\n",
      "FLORIZEL:\n",
      "We then should not see your father vexations.\n",
      "\n",
      "CAMILLO:\n",
      "The senators is, in some patient third--\n",
      "\n",
      "Provator:\n",
      "Bid them well secohe st are then robes to Angelo?--\n",
      "\n",
      "MAMILLIA:\n",
      "Be never give better countend to live mock\n",
      "of your knees vent-on a young brother till\n",
      "you shall be deted for my on yours.\n",
      "\n",
      "LEONTES:\n",
      "'For this same sighs of four; an yet, the throat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aea5ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T09:05:01.251325Z",
     "iopub.status.busy": "2023-07-06T09:05:01.250396Z",
     "iopub.status.idle": "2023-07-06T09:06:35.001904Z",
     "shell.execute_reply": "2023-07-06T09:06:35.000994Z"
    },
    "papermill": {
     "duration": 93.790658,
     "end_time": "2023-07-06T09:06:35.021008",
     "exception": false,
     "start_time": "2023-07-06T09:05:01.230350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Officious, and the people of the fire;\n",
      "And him with his virtuous natural words;\n",
      "The sword of a parageit and confirmate\n",
      "Where it had our courtesy: nor my garms should we,\n",
      "For shame, but defy him-shamed with the gentle hours\n",
      "That sweet's infant; and so retoris as it glow,\n",
      "To help misforthy, every known, which shows\n",
      "In commissingly strength; for near nature wears, each in\n",
      "joy.\n",
      "I am enten'd to no good fellow, health I wear:\n",
      "Ah, tell-my mother may stone return to me;\n",
      "And leave me have this country'd in press,\n",
      "From burnish up twenty timeline arms,\n",
      "And think it and thousand lay in his afety.\n",
      "Kninghough, steal away, let make him for joy.\n",
      "\n",
      "Page:\n",
      "Give me, pretty mother.\n",
      "\n",
      "Servant:\n",
      "\n",
      "MONTAGUE:\n",
      "Tough the duke no more but when twenty to death.\n",
      "\n",
      "Shephere:\n",
      "For the noble same wanished, his head said we came:\n",
      "Some polishmen to give both the roly sun,\n",
      "To high father for a linen's steed consequence\n",
      "Which by things prown, the king I, till therefore.\n",
      "Now, as if my attention came the king,\n",
      "Short'd his reeky a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n",
    "open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f80758e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T09:06:35.061406Z",
     "iopub.status.busy": "2023-07-06T09:06:35.060376Z",
     "iopub.status.idle": "2023-07-06T09:06:36.046490Z",
     "shell.execute_reply": "2023-07-06T09:06:36.045276Z"
    },
    "papermill": {
     "duration": 1.008974,
     "end_time": "2023-07-06T09:06:36.049290",
     "exception": false,
     "start_time": "2023-07-06T09:06:35.040316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "That very gine, country or pick. How intelligence!\r\n",
      "Ethink you and begin our place, bidges\r\n",
      "Eve calm them to a battle moon, and we\r\n",
      "Deny their lawful time to come; of thine away,\r\n",
      "Though we doubt, yet might with the lice, which amain\r\n",
      "I so lies unfollient; finds him, a sleep,\r\n",
      "And crucks about schangeth one air and in things\r\n",
      "Lreaden jladem are forerut and scared free.\r\n",
      "What, they have not foughler frowned by Edward's sland:\r\n",
      "And wear more than pity, than fearing their land\r\n",
      "KING Rowhisper enemies at the table power'd where:\r\n",
      "For their out-dear courteom from thought the trumpet,\r\n",
      "Not gaving that twenty headlong in view,\r\n",
      "Inthrallings in self-born to thy eyes,\r\n",
      "Like red-evrising bloody dressed by--\r\n",
      "Which to his pravil in the surpest scorn,\r\n",
      "To scorns him pleased towadful some-place;\r\n",
      "'Making plood the like to slep his fault well,\r\n",
      "Displotten fierce fled the hour in struth,\r\n",
      "To breathe no love that to our blows upon,\r\n",
      "In reilike or apprivate potents\r\n",
      "Are number only conquectoer to serve my father.\r\n",
      "\r\n",
      "First Servant:\r\n",
      "Not adward with her ghostless that makes thee,\r\n",
      "but farewell as surred that my breathed keeps.\r\n",
      "\r\n",
      "First Servingman:\r\n",
      "Say 'Tis very none ere too lately here.\r\n",
      "\r\n",
      "Lord:\r\n",
      "Ay, my lords. Gaod; what news abrocks?\r\n",
      "\r\n",
      "Second Servant:\r\n",
      "A bank-fond, what say is, thy lief a man?\r\n",
      "\r\n",
      "STANLEY:\r\n",
      "My lords more, abovel; the senate hath forceded\r\n",
      "The way crosse of our beauty, and says i' the dog.\r\n",
      "\r\n",
      "Like Roman:\r\n",
      "So did, not the strange of a smole house.\r\n",
      "\r\n",
      "Lord:\r\n",
      "I think it is beseeming; whils I say. Alas, in this time\r\n",
      "Is known eckoless of that king.\r\n",
      "\r\n",
      "CLIFFORD:\r\n",
      "What do you think in this descent?\r\n",
      "\r\n",
      "Son:\r\n",
      "Nor in his sacred shift have saintly sad\r\n",
      "Full like an irrer brach from Duke of sac\r\n",
      "Comisgulong, judge, and noble fights of the god\r\n",
      "and the deed us dearer is the tewY gallant-sea\r\n",
      "of the earth with the king shephinks fall acquest fortune;\r\n",
      "For eafter we even as there, as we long their here their\r\n",
      "assaking them to be reasolved with us ingod\r\n",
      "towards; and for the poures of the wind\r\n",
      "breadful poor punishment. The one poor lowling who being\r\n",
      "one that dwould poor Meneland Duke of York\r\n",
      "noright having alive and humour to his parts in health.\r\n",
      "\r\n",
      "ESCALUS:\r\n",
      "Do not mock my lock with his quastion?\r\n",
      "\r\n",
      "DUKE VINCENTIO:\r\n",
      "At our trickly; not implot him so was to rare,\r\n",
      "If he found the stands of our rose-broken law.\r\n",
      "\r\n",
      "ANGELO:\r\n",
      "Well, I say. Fear not it; for that please so, ourself.\r\n",
      "\r\n",
      "DUKE VINCENTIO:\r\n",
      "More news thanks your knave. Thirt gentleman,\r\n",
      "I may give you for your hand bought with your country.\r\n",
      "Second Second Lord Oxford, this in sease,\r\n",
      "For so you presumes him that part 'banished;'\r\n",
      "For, like hand, would hold be the oath live,\r\n",
      "Likewing about my dardhering by her lust,\r\n",
      "And in the false complish outwains,\r\n",
      "Where strong like sound yet shall prodegr\r\n",
      "To have reason, and 't good lady. But, soft!\r\n",
      "Welcome brave to it, and be done to visit\r\n",
      "O him false that is nothing repredeem:\r\n",
      "Nor yok we, what thy brother will never they grow.\r\n",
      "The sun fame his name shows arm at thy remetasing;\r\n",
      "Which then thou wouldst bar but the early looking\r\n",
      "Twice on the other-bed ign-tongue,\r\n",
      "For as mine angled through swell of death,\r\n",
      "Thou, king, from trembling, fiercement sorted wind.\r\n",
      "\r\n",
      "Nurse:\r\n",
      "O thou that all of your grafe, hath adowrn'd the ground;\r\n",
      "The innocess affection and takes a gross them crown;\r\n",
      "But very wronged Cleave to man, who bids' hate once it,\r\n",
      "Reven'd in his voice of earth, and break this shall,\r\n",
      "From short with a flower colution to heel\r\n",
      "Idown by profan accustors, band never,\r\n",
      "May not be know like a rapise of sea?\r\n",
      "Was sink you from his death?\r\n",
      "Methinks Clareliam to bear amends to dissover his land,\r\n",
      "And came would betwo wash King of London,\r\n",
      "Have we did weep and show To our denouncing hels!\r\n",
      "The heels of thou deckwarding courate a brow,\r\n",
      "And inclise have I woo'd a man, or in bidsh war,\r\n",
      "That breath ascent cled as befreignly in mine\r\n",
      "Hastiling mortality bey'd back time,\r\n",
      "His springs aboutmity, will wrought love these\r\n",
      "To make this afristious right's neck,\r\n",
      "Answer me unto the fiery sovereigns\r\n",
      "His wazing of visital royal of thy lowledge,\r\n",
      "Which she shall prompts us will the earth and ear\r\n",
      "O'er wond suckingh of his court-a.\r\n",
      "Thou newsed Norfolk whilst to Richmond,\r\n",
      "But not a half, thou, or I am gilty, who is\r\n",
      "Then fashion enter to many more mine adortilies,\r\n",
      "With too execution reconceition; what thou think'st\r\n",
      "But that honesty treason will take down\r\n",
      "As sometimes can cet the stuff quicker off fear\r\n",
      "More to make the vaultter hath set a gress,\r\n",
      "Since my knees cut on the droop.\r\n",
      "\r\n",
      "BENVOLIO:\r\n",
      "Come, entermanning was, the field wind.\r\n",
      "\r\n",
      "ROMEO:\r\n",
      "My master hears your gage's hath air your hands;\r\n",
      "Your grandam play of late, if you should show\r\n",
      "Becase your font faults, you be refll,\r\n",
      "Wear't picture me; until trumpets; call the words\r\n",
      "Take breath the nubbinging that the one.\r\n",
      "Heavy age, multting nothing joy!\r\n",
      "\r\n",
      "BENVOLIO:\r\n",
      "Yond queen and three things. What pain they have cause\r\n",
      "To wreak out of Erevius, who being overtowed\r\n",
      "With ropen my couts of forceful justice,\r\n",
      "Bound upon the made your fancy: which would it were\r\n",
      "Thy nature intent did formel time unto\r\n",
      "The advantagency butterf"
     ]
    }
   ],
   "source": [
    "!cat more.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aab05b3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T09:06:36.092263Z",
     "iopub.status.busy": "2023-07-06T09:06:36.090715Z",
     "iopub.status.idle": "2023-07-06T09:06:36.096390Z",
     "shell.execute_reply": "2023-07-06T09:06:36.095487Z"
    },
    "papermill": {
     "duration": 0.029021,
     "end_time": "2023-07-06T09:06:36.098556",
     "exception": false,
     "start_time": "2023-07-06T09:06:36.069535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bcca2",
   "metadata": {
    "papermill": {
     "duration": 0.019115,
     "end_time": "2023-07-06T09:06:36.137155",
     "exception": false,
     "start_time": "2023-07-06T09:06:36.118040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c25aed40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T09:06:36.177862Z",
     "iopub.status.busy": "2023-07-06T09:06:36.177525Z",
     "iopub.status.idle": "2023-07-06T09:06:36.428379Z",
     "shell.execute_reply": "2023-07-06T09:06:36.427407Z"
    },
    "papermill": {
     "duration": 0.275579,
     "end_time": "2023-07-06T09:06:36.432418",
     "exception": false,
     "start_time": "2023-07-06T09:06:36.156839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 384)\n",
      "  (position_embedding_table): Embedding(256, 384)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(model, open('model.pkl', 'wb'))\n",
    "\n",
    "\n",
    "# Load the model from the pickle file\n",
    "model = pickle.load(open('/kaggle/working/model.pkl', 'rb'))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3b71c0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T09:06:36.473714Z",
     "iopub.status.busy": "2023-07-06T09:06:36.473413Z",
     "iopub.status.idle": "2023-07-06T09:06:36.479987Z",
     "shell.execute_reply": "2023-07-06T09:06:36.479111Z"
    },
    "papermill": {
     "duration": 0.029467,
     "end_time": "2023-07-06T09:06:36.482324",
     "exception": false,
     "start_time": "2023-07-06T09:06:36.452857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 10788929\n"
     ]
    }
   ],
   "source": [
    "# Print the number of parameters in the model\n",
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of parameters:\", num_parameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2154.69134,
   "end_time": "2023-07-06T09:06:39.089611",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-06T08:30:44.398271",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
